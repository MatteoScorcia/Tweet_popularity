{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'emoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-23f4f8f31f28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'emoji'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/raw_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars, question_marks, esclamation_marks, emojis, hashtags, tags, urls, pos_count  = [], [], [], [], [], [], [], []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    tweet = row.text\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    nr_question_marks, nr_esclamation_marks = 0, 0\n",
    "\n",
    "    # question and esclamation marks\n",
    "    for token in tokens:\n",
    "        if token == '?':\n",
    "            nr_question_marks += 1\n",
    "        if token == '!':\n",
    "            nr_esclamation_marks += 1\n",
    "\n",
    "    # emoji\n",
    "    allchars = [str for str in tweet]\n",
    "    lista = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    nr_emoji = len(lista)\n",
    "    for c in tweet:\n",
    "        if c in lista:\n",
    "            tweet = tweet.replace(c, \"\")\n",
    "\n",
    "    # hashtags\n",
    "    try:\n",
    "        nr_hashtags = len(re.findall(r\"#(\\w+)\", tweet))\n",
    "        tweet = re.sub(r\"#(\\w+)\", \"\", tweet, count=nr_hashtags)\n",
    "    except Exception:\n",
    "        nr_hashtags = 0\n",
    "\n",
    "\n",
    "    # tags\n",
    "    try:\n",
    "        nr_tags = len(re.findall(r\" @(\\w+)\", tweet))\n",
    "        tweet = re.sub(r\" @(\\w+)\", \"\", tweet, count=nr_tags)\n",
    "    except Exception:\n",
    "        nr_tags = 0\n",
    "\n",
    "    # urls\n",
    "    try:\n",
    "        nr_urls = len(re.findall(r\"http[s]?://([a-zA-Z0-9/.]+)\", tweet))\n",
    "        tweet = re.sub(r\"http[s]?://([a-zA-Z0-9/.]+)\", \"\", tweet, count=nr_urls)\n",
    "    except Exception:\n",
    "        nr_urls = 0\n",
    "        \n",
    "    # special characters\n",
    "    tweet = re.sub('[^A-Za-z0-9 ]+', '', tweet)\n",
    "    \n",
    "    #POS TAGGING\n",
    "    _UNIVERSAL_TAGS = (\n",
    "    \"VERB\",\n",
    "    \"NOUN\",\n",
    "    \"ADJ\",\n",
    "    \"ADV\"\n",
    "    )\n",
    "    \n",
    "    pos = nltk.pos_tag(tweet.split())\n",
    "    simplifiedPos = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in pos]\n",
    "    \n",
    "    universal_tags_list = list(_UNIVERSAL_TAGS)\n",
    "    count = []\n",
    "    for i in range(0, len(universal_tags_list)):\n",
    "        count.append(0)\n",
    "    \n",
    "    for element in simplifiedPos:\n",
    "        for index, target in enumerate(universal_tags_list):\n",
    "            if target==element[1]:\n",
    "                count[index] += 1\n",
    "    pos_count.append(count)\n",
    "    \n",
    "    # no spaces\n",
    "    try:\n",
    "        tweet = re.sub(\" \", \"\", tweet, len(re.findall(\" \", tweet)))\n",
    "    except Exception:\n",
    "        \"Error: spaces have not be deleted from the tweet.\"\n",
    "    \n",
    "    chars.append(len(tweet))\n",
    "    question_marks.append(nr_question_marks)\n",
    "    esclamation_marks.append(nr_esclamation_marks)\n",
    "    emojis.append(nr_emoji)\n",
    "    hashtags.append(nr_hashtags)\n",
    "    tags.append(nr_tags)\n",
    "    urls.append(nr_urls)\n",
    "\n",
    "df['plain_text_len'] = chars\n",
    "df['question_marks'] = question_marks\n",
    "df['esclamation_marks'] = esclamation_marks\n",
    "df['emojis'] = emojis\n",
    "df['hashtags'] = hashtags\n",
    "df['tags'] = tags\n",
    "df['urls'] = urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(pos_count, columns=universal_tags_list)\n",
    "df.drop(columns=[\"ratio\", \"date\", \"text\"], inplace=True)\n",
    "\n",
    "result = pd.concat([df, df1], axis=1, sort=False)\n",
    "\n",
    "#rename columns\n",
    "result.rename(columns = {'VERB': 'verbs', 'NOUN': 'nouns', 'ADJ' : 'adjs', 'ADV' : 'advs'}, inplace = True)\n",
    "\n",
    "result.to_csv(\"../dataset/dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../dataset/raw_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Living_Lou         220\n",
       "CookGlobal         158\n",
       "KitchenSanc2ary    148\n",
       "Allrecipes         115\n",
       "deliciousmag       109\n",
       "RecipesIdeas        61\n",
       "seriouseats         58\n",
       "misspickledplum     26\n",
       "CookingChannel      20\n",
       "indianclaypot        7\n",
       "FoodizShare          6\n",
       "Name: screen_name, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"screen_name\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "928"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"screen_name\"].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
